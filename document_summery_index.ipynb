{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, Document, SummaryIndex\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "import nest_asyncio\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\"Toronto\", \"Seattle\", \"Chicago\", \"Boston\", \"Houston\"]\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            # 'exintro': True,\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all wiki documents\n",
    "city_docs = []\n",
    "for wiki_title in wiki_titles:\n",
    "    docs = SimpleDirectoryReader(input_files=[f\"data/{wiki_title}.txt\"]).load_data()\n",
    "    docs[0].doc_id = wiki_title\n",
    "    city_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (gpt-3.5-turbo)\n",
    "chatgpt = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "service_context = ServiceContext.from_defaults(llm=chatgpt, chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents into nodes: 100%|██████████| 5/5 [00:00<00:00, 18.24it/s]\n",
      "Summarizing documents:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: Toronto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing documents:  20%|██        | 1/5 [02:55<11:40, 175.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: Seattle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing documents:  40%|████      | 2/5 [05:10<07:34, 151.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: Chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing documents:  60%|██████    | 3/5 [07:34<04:56, 148.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: Boston\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing documents:  60%|██████    | 3/5 [08:52<05:54, 177.36s/it]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for gpt-3.5-turbo in organization org-aLklZtqyIUfXeSgPUXAPHAaB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# default mode of building the index\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m response_synthesizer \u001b[39m=\u001b[39m get_response_synthesizer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtree_summarize\u001b[39m\u001b[39m\"\u001b[39m, use_async\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m doc_summary_index \u001b[39m=\u001b[39m DocumentSummaryIndex\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     city_docs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     response_synthesizer\u001b[39m=\u001b[39;49mresponse_synthesizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     show_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/base.py:102\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     docstore\u001b[39m.\u001b[39mset_document_hash(doc\u001b[39m.\u001b[39mget_doc_id(), doc\u001b[39m.\u001b[39mhash)\n\u001b[1;32m     98\u001b[0m nodes \u001b[39m=\u001b[39m service_context\u001b[39m.\u001b[39mnode_parser\u001b[39m.\u001b[39mget_nodes_from_documents(\n\u001b[1;32m     99\u001b[0m     documents, show_progress\u001b[39m=\u001b[39mshow_progress\n\u001b[1;32m    100\u001b[0m )\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    103\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    104\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m    105\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m    106\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/document_summary/base.py:89\u001b[0m, in \u001b[0;36mDocumentSummaryIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, storage_context, response_synthesizer, summary_query, show_progress, embed_summaries, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_query \u001b[39m=\u001b[39m summary_query\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embed_summaries \u001b[39m=\u001b[39m embed_summaries\n\u001b[0;32m---> 89\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     90\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m     91\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     92\u001b[0m     service_context\u001b[39m=\u001b[39;49mservice_context,\n\u001b[1;32m     93\u001b[0m     storage_context\u001b[39m=\u001b[39;49mstorage_context,\n\u001b[1;32m     94\u001b[0m     show_progress\u001b[39m=\u001b[39;49mshow_progress,\n\u001b[1;32m     95\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     96\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/base.py:71\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m index_struct \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[39massert\u001b[39;00m nodes \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     index_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_index_from_nodes(nodes)\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct \u001b[39m=\u001b[39m index_struct\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_storage_context\u001b[39m.\u001b[39mindex_store\u001b[39m.\u001b[39madd_index_struct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/base.py:171\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_docstore\u001b[39m.\u001b[39madd_documents(nodes, allow_update\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_index_from_nodes(nodes)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/document_summary/base.py:213\u001b[0m, in \u001b[0;36mDocumentSummaryIndex._build_index_from_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m# first get doc_id to nodes_dict, generate a summary for each doc_id,\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m# then build the index struct\u001b[39;00m\n\u001b[1;32m    212\u001b[0m index_struct \u001b[39m=\u001b[39m IndexDocumentSummary()\n\u001b[0;32m--> 213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_nodes_to_index(index_struct, nodes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_show_progress)\n\u001b[1;32m    214\u001b[0m \u001b[39mreturn\u001b[39;00m index_struct\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/document_summary/base.py:172\u001b[0m, in \u001b[0;36mDocumentSummaryIndex._add_nodes_to_index\u001b[0;34m(self, index_struct, nodes, show_progress)\u001b[0m\n\u001b[1;32m    170\u001b[0m nodes_with_scores \u001b[39m=\u001b[39m [NodeWithScore(node\u001b[39m=\u001b[39mn) \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nodes]\n\u001b[1;32m    171\u001b[0m \u001b[39m# get the summary for each doc_id\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m summary_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    173\u001b[0m     query\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_summary_query,\n\u001b[1;32m    174\u001b[0m     nodes\u001b[39m=\u001b[39;49mnodes_with_scores,\n\u001b[1;32m    175\u001b[0m )\n\u001b[1;32m    176\u001b[0m summary_response \u001b[39m=\u001b[39m cast(Response, summary_response)\n\u001b[1;32m    177\u001b[0m summary_node_dict[doc_id] \u001b[39m=\u001b[39m TextNode(\n\u001b[1;32m    178\u001b[0m     text\u001b[39m=\u001b[39msummary_response\u001b[39m.\u001b[39mresponse,\n\u001b[1;32m    179\u001b[0m     relationships\u001b[39m=\u001b[39m{\n\u001b[1;32m    180\u001b[0m         NodeRelationship\u001b[39m.\u001b[39mSOURCE: RelatedNodeInfo(node_id\u001b[39m=\u001b[39mdoc_id)\n\u001b[1;32m    181\u001b[0m     },\n\u001b[1;32m    182\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:140\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    135\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    138\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    139\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 140\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    141\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    142\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    143\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    144\u001b[0m         ],\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    148\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/response_synthesizers/tree_summarize.py:150\u001b[0m, in \u001b[0;36mTreeSummarize.get_response\u001b[0;34m(self, query_str, text_chunks, **response_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async:\n\u001b[1;32m    141\u001b[0m     tasks \u001b[39m=\u001b[39m [\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mapredict(\n\u001b[1;32m    143\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    148\u001b[0m     ]\n\u001b[0;32m--> 150\u001b[0m     summaries: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m run_async_tasks(tasks)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     summaries \u001b[39m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    154\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    159\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/async_utils.py:38\u001b[0m, in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n\u001b[0;32m---> 38\u001b[0m outputs: List[Any] \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(_gather())\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[1;32m     32\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\u001b[39m.\u001b[39mwith_traceback(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49mthrow(exc)\n\u001b[1;32m    235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_must_cancel:\n\u001b[1;32m    237\u001b[0m         \u001b[39m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/async_utils.py:36\u001b[0m, in \u001b[0;36mrun_async_tasks.<locals>._gather\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__wakeup\u001b[39m(\u001b[39mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    305\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[39m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:226\u001b[0m, in \u001b[0;36mLLMPredictor.apredict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    224\u001b[0m     messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_messages(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    225\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_messages(messages)\n\u001b[0;32m--> 226\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm\u001b[39m.\u001b[39machat(messages)\n\u001b[1;32m    227\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/base.py:108\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m     99\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    100\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    101\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         },\n\u001b[1;32m    106\u001b[0m     )\n\u001b[0;32m--> 108\u001b[0m     f_return_val \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m f(_self, messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, AsyncGenerator):\n\u001b[1;32m    110\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    111\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseAsyncGen:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai.py:363\u001b[0m, in \u001b[0;36mOpenAI.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     achat_fn \u001b[39m=\u001b[39m acompletion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acomplete)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m achat_fn(messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai.py:404\u001b[0m, in \u001b[0;36mOpenAI._achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_achat\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[39mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    402\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponse:\n\u001b[1;32m    403\u001b[0m     message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 404\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    405\u001b[0m         is_chat_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    406\u001b[0m         max_retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    407\u001b[0m         messages\u001b[39m=\u001b[39mmessage_dicts,\n\u001b[1;32m    408\u001b[0m         stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    409\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs),\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m     message_dict \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    412\u001b[0m     message \u001b[39m=\u001b[39m from_openai_message_dict(message_dict)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:154\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(is_chat_model, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[1;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 154\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_wrapped\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:152\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    150\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:219\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39macreate\u001b[39m(\n\u001b[1;32m    196\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    204\u001b[0m ):\n\u001b[1;32m    205\u001b[0m     (\n\u001b[1;32m    206\u001b[0m         deployment_id,\n\u001b[1;32m    207\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 219\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m requestor\u001b[39m.\u001b[39marequest(\n\u001b[1;32m    220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         url,\n\u001b[1;32m    222\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    223\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    224\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    225\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    226\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:384\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marequest_raw(\n\u001b[1;32m    375\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    376\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    383\u001b[0m     )\n\u001b[0;32m--> 384\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[1;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39m# Close the request before exiting session context.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:738\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m     util\u001b[39m.\u001b[39mlog_warn(e, body\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m    737\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    739\u001b[0m         (\u001b[39mawait\u001b[39;49;00m result\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    740\u001b[0m         result\u001b[39m.\u001b[39;49mstatus,\n\u001b[1;32m    741\u001b[0m         result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    742\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    743\u001b[0m     ),\n\u001b[1;32m    744\u001b[0m     \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    745\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for gpt-3.5-turbo in organization org-aLklZtqyIUfXeSgPUXAPHAaB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "# default mode of building the index\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    city_docs,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided text is about the city of Boston, Massachusetts, and covers various aspects of the city including its history, geography, climate, neighborhoods, demographics, economy, education system, healthcare facilities, public safety, culture, environment, government and politics, media, and infrastructure. It provides information on Boston's development over time, key events during the American Revolution, its significance in terms of education and academic research, economic sectors contributing to its economy, changes and evolution in the 20th and 21st centuries, geography and its impact on the city, climate, neighborhoods, demographic breakdown, major industries, religious composition, population changes over time, economic significance in the global context, landmarks, ethnic diversity, income levels, and major universities and colleges. It also discusses Boston's healthcare system, public safety, cultural scene, environmental initiatives, churches, air quality, water purity and availability, climate change and sea level rise, sports teams and championships, parks and recreational areas, government structure, newspapers, radio and television stations, films set in Boston, video games featuring Boston, transportation infrastructure, airports, highways, public transit, biking options, international relations, sister cities, and partnership relationships.\\n\\nSome questions that this text can answer include:\\n- What is the history of Boston and how did it develop over time?\\n- What were some key events that took place in Boston during the American Revolution?\\n- What is the significance of Boston in terms of education and academic research?\\n- What are some of the economic sectors that contribute to Boston's economy?\\n- How has Boston changed and evolved in the 20th and 21st centuries?\\n- What is the geography of Boston and how does it impact the city?\\n- What are some of the neighborhoods in Boston?\\n- What is the climate like in Boston?\\n- What is the demographic breakdown of the city?\\n- What are the major industries in Boston?\\n- What is the religious composition of the city?\\n- How has the population of Boston changed over time?\\n- What is the economic significance of Boston in the global context?\\n- What are some of the major universities and colleges in Boston?\\n- What are the prominent healthcare facilities and medical institutions in the city?\\n- How does Boston ensure public safety and what is the crime rate like?\\n- What is the cultural scene like in Boston and what are some of the notable events and institutions?\\n- What environmental initiatives and policies are in place in Boston?\\n- What are some of the oldest churches in Boston?\\n- How is air quality in Boston?\\n- What is the state of water purity and availability in the city?\\n- What is Boston's climate action plan?\\n- Which sports teams are based in Boston and what championships have they won?\\n- What are some of the major parks and recreational areas in the city?\\n- How is the government structured in Boston?\\n- What are some of the newspapers, radio stations, and television stations in the city?\\n- What films and video games are set in Boston?\\n- What is the transportation infrastructure like in Boston?\\n- What are the major airports in Boston?\\n- What is the public transit system like in Boston?\\n- What are some of Boston's sister cities and partnership relationships?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_summary_index.get_document_summary(\"Boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_index.storage_context.persist(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.loading import load_index_from_storage\n",
    "from llama_index import StorageContext\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index\")\n",
    "doc_summary_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Retrieval from Document Summary Index\n",
    "** High-level Querying **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for gpt-3.5-turbo in organization org-aLklZtqyIUfXeSgPUXAPHAaB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m query_engine \u001b[39m=\u001b[39m doc_summary_index\u001b[39m.\u001b[39mas_query_engine(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     response_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtree_summarize\u001b[39m\u001b[39m\"\u001b[39m, use_async\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m response \u001b[39m=\u001b[39m query_engine\u001b[39m.\u001b[39;49mquery(\u001b[39m\"\u001b[39;49m\u001b[39mWhat are the sports teams in Toronto?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/himalaypatel/Documents/HimalayWork/Learning/LlamaIndex/document_summery_index.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/indices/query/base.py:23\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     22\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 23\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:177\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    171\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretrieve(query_bundle)\n\u001b[1;32m    173\u001b[0m         retrieve_event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    174\u001b[0m             payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 177\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_response_synthesizer\u001b[39m.\u001b[39;49msynthesize(\n\u001b[1;32m    178\u001b[0m         query\u001b[39m=\u001b[39;49mquery_bundle,\n\u001b[1;32m    179\u001b[0m         nodes\u001b[39m=\u001b[39;49mnodes,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    184\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/response_synthesizers/base.py:140\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes)\u001b[0m\n\u001b[1;32m    135\u001b[0m     query \u001b[39m=\u001b[39m QueryBundle(query_str\u001b[39m=\u001b[39mquery)\n\u001b[1;32m    137\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    138\u001b[0m     CBEventType\u001b[39m.\u001b[39mSYNTHESIZE, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    139\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 140\u001b[0m     response_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_response(\n\u001b[1;32m    141\u001b[0m         query_str\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mquery_str,\n\u001b[1;32m    142\u001b[0m         text_chunks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    143\u001b[0m             n\u001b[39m.\u001b[39;49mnode\u001b[39m.\u001b[39;49mget_content(metadata_mode\u001b[39m=\u001b[39;49mMetadataMode\u001b[39m.\u001b[39;49mLLM) \u001b[39mfor\u001b[39;49;00m n \u001b[39min\u001b[39;49;00m nodes\n\u001b[1;32m    144\u001b[0m         ],\n\u001b[1;32m    145\u001b[0m     )\n\u001b[1;32m    147\u001b[0m     additional_source_nodes \u001b[39m=\u001b[39m additional_source_nodes \u001b[39mor\u001b[39;00m []\n\u001b[1;32m    148\u001b[0m     source_nodes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(nodes) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/response_synthesizers/tree_summarize.py:150\u001b[0m, in \u001b[0;36mTreeSummarize.get_response\u001b[0;34m(self, query_str, text_chunks, **response_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async:\n\u001b[1;32m    141\u001b[0m     tasks \u001b[39m=\u001b[39m [\n\u001b[1;32m    142\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mapredict(\n\u001b[1;32m    143\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    148\u001b[0m     ]\n\u001b[0;32m--> 150\u001b[0m     summaries: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m run_async_tasks(tasks)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     summaries \u001b[39m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39mllm_predictor\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m    154\u001b[0m             summary_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[39mfor\u001b[39;00m text_chunk \u001b[39min\u001b[39;00m text_chunks\n\u001b[1;32m    159\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/async_utils.py:38\u001b[0m, in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n\u001b[0;32m---> 38\u001b[0m outputs: List[Any] \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39;49mrun(_gather())\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m loop\u001b[39m.\u001b[39;49mrun_until_complete(task)\n\u001b[1;32m     32\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m task\u001b[39m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEvent loop stopped before Future completed.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mresult()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__log_traceback \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\u001b[39m.\u001b[39mwith_traceback(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:234\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49mthrow(exc)\n\u001b[1;32m    235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    236\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_must_cancel:\n\u001b[1;32m    237\u001b[0m         \u001b[39m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/async_utils.py:36\u001b[0m, in \u001b[0;36mrun_async_tasks.<locals>._gather\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Any]:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39mtasks_to_execute)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__wakeup\u001b[39m(\u001b[39mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    305\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[39m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llm_predictor/base.py:226\u001b[0m, in \u001b[0;36mLLMPredictor.apredict\u001b[0;34m(self, prompt, output_cls, **prompt_args)\u001b[0m\n\u001b[1;32m    224\u001b[0m     messages \u001b[39m=\u001b[39m prompt\u001b[39m.\u001b[39mformat_messages(llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprompt_args)\n\u001b[1;32m    225\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_messages(messages)\n\u001b[0;32m--> 226\u001b[0m     chat_response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_llm\u001b[39m.\u001b[39machat(messages)\n\u001b[1;32m    227\u001b[0m     output \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/base.py:108\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m     99\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    100\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    101\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         },\n\u001b[1;32m    106\u001b[0m     )\n\u001b[0;32m--> 108\u001b[0m     f_return_val \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m f(_self, messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, AsyncGenerator):\n\u001b[1;32m    110\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    111\u001b[0m         \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponseAsyncGen:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai.py:363\u001b[0m, in \u001b[0;36mOpenAI.achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     achat_fn \u001b[39m=\u001b[39m acompletion_to_chat_decorator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_acomplete)\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m achat_fn(messages, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai.py:404\u001b[0m, in \u001b[0;36mOpenAI._achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_achat\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[39mself\u001b[39m, messages: Sequence[ChatMessage], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m    402\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResponse:\n\u001b[1;32m    403\u001b[0m     message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[0;32m--> 404\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m acompletion_with_retry(\n\u001b[1;32m    405\u001b[0m         is_chat_model\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    406\u001b[0m         max_retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    407\u001b[0m         messages\u001b[39m=\u001b[39mmessage_dicts,\n\u001b[1;32m    408\u001b[0m         stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    409\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs),\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m     message_dict \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    412\u001b[0m     message \u001b[39m=\u001b[39m from_openai_message_dict(message_dict)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:154\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[0;34m(is_chat_model, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[1;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 154\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[1;32m     87\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_wrapped\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[1;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[0;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[0;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/tenacity/_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m     49\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/llama_index/llms/openai_utils.py:152\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    149\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    150\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m client\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:219\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39macreate\u001b[39m(\n\u001b[1;32m    196\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    204\u001b[0m ):\n\u001b[1;32m    205\u001b[0m     (\n\u001b[1;32m    206\u001b[0m         deployment_id,\n\u001b[1;32m    207\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 219\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m requestor\u001b[39m.\u001b[39marequest(\n\u001b[1;32m    220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         url,\n\u001b[1;32m    222\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    223\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    224\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[1;32m    225\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[1;32m    226\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:384\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marequest_raw(\n\u001b[1;32m    375\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    376\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    383\u001b[0m     )\n\u001b[0;32m--> 384\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[1;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[39m# Close the request before exiting session context.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:738\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_async_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39mexcept\u001b[39;00m aiohttp\u001b[39m.\u001b[39mClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m     util\u001b[39m.\u001b[39mlog_warn(e, body\u001b[39m=\u001b[39mresult\u001b[39m.\u001b[39mcontent)\n\u001b[1;32m    737\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    739\u001b[0m         (\u001b[39mawait\u001b[39;49;00m result\u001b[39m.\u001b[39;49mread())\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    740\u001b[0m         result\u001b[39m.\u001b[39;49mstatus,\n\u001b[1;32m    741\u001b[0m         result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    742\u001b[0m         stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    743\u001b[0m     ),\n\u001b[1;32m    744\u001b[0m     \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    745\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HimalayWork/Learning/LlamaIndex/env/lib/python3.10/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for gpt-3.5-turbo in organization org-aLklZtqyIUfXeSgPUXAPHAaB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method."
     ]
    }
   ],
   "source": [
    "query_engine = doc_summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\", use_async=True\n",
    ")\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "retriever = DocumentSummaryIndexLLMRetriever(\n",
    "    doc_summary_index,\n",
    "    # choice_select_prompt=None,\n",
    "    # choice_batch_size=10,\n",
    "    # choice_top_k=1,\n",
    "    # format_node_batch_fn=None,\n",
    "    # parse_choice_select_answer_fn=None,\n",
    "    # service_context=None\n",
    ")\n",
    "retrieved_nodes = retriever.retrieve(\"What are the sports teams in Toronto?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retrieved_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_nodes[0].score)\n",
    "print(retrieved_nodes[0].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retriever as part of a query engine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding-based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.document_summary import DocumentSummaryIndexEmbeddingRetriever\n",
    "retriever = DocumentSummaryIndexEmbeddingRetriever(\n",
    "    doc_summary_index,\n",
    "    # similarity_top_k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(\"What are the sports teams in Toronto?\")\n",
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_nodes[0].node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use retriever as part of a query engine\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(response_mode=\"tree_summarize\")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What are the sports teams in Toronto?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
